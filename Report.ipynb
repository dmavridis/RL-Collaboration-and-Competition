{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and competition project report\n",
    "## Introduction\n",
    "\n",
    "In this project the Tennis environment is used to train two agents to cooperate. \n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation.  Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "The task is episodic, and in order to solve the environment, your agents must get an average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents). Specifically,\n",
    "\n",
    "- After each episode, we add up the rewards that each agent received (without discounting), to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores.\n",
    "- This yields a single **score** for each episode.\n",
    "\n",
    "The environment is considered solved, when the average (over 100 episodes) of those **scores** is at least +0.5.\n",
    "\n",
    "A sample of the untrained agents looks like:\n",
    "\n",
    "[<img src=\"images/tennis_untrained.gif\">]()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Approach to the problem\n",
    "\n",
    "The environment is a multiagent one where the cooperation of agents is required to achieve the target. The agents are implementing the DDPG algorithm and each experience is added to a common buffer. \n",
    "\n",
    "\n",
    "### Agent\n",
    "Each of the agents consists of an actor and a critic, both of which are implemented as Q networks with local and target networks.\n",
    "\n",
    "The noise process is a Ornstein-Uhlenbeck one where the σ value has been changed and proved during the training to be an important parameter to achieve solution.\n",
    "\n",
    "### Structure of the solution\n",
    "The solution is implemented in `tennis.py` file. The structure of the project is the follwoing:\n",
    "\n",
    "- `tennis.py`: Training implementation\n",
    "- `code/maddpg.py`: Algorithm implementation for the multiagent environment\n",
    "- `code/ddpg.py`: Single agent implementation\n",
    "- `code/model.py`: Model neural network definition of the agents\n",
    "- `code/utils.py`: Definition of noise and replay buffer required for the agents\n",
    "- `models/*`: Saved models that achieve the target score\n",
    "- `tennis_untrained.py`: Shows the untrained agents interacting (helper file to capture video)\n",
    "- `tennis_trained.py`: Shows the trained agents interacting (helper file to capture video)\n",
    "\n",
    "\n",
    "### Neural network model\n",
    "The Neural Network models for the Actor and the Critic are 3-hidden layer fully connected layer neural networks, consisting of `512`, `256` and `128` units respectively. The output layer for the actor is `tanh` and the outputs are clipped in the range `[-1, +1]`. Other values can also achieve the result therefore there is a wide range of number to try.\n",
    "\n",
    "### Hyperparameters\n",
    "The following hyperparameters are set \n",
    "\n",
    "| Name        | Value |\n",
    "|-------------|-------|\n",
    "| BUFFER_SIZE | 1e5   |\n",
    "| BATCH_SIZE  | 256   |\n",
    "| GAMMA       | 0.99  |\n",
    "| TAU         | 1e-3  |\n",
    "| LR_ACTOR    | 3e-5  |\n",
    "| LR_CRITIC   | 1e-4  |\n",
    "| NOISE σ     | 0.11  |\n",
    "\n",
    "\n",
    "\n",
    "## Training and plot of rewards\n",
    "The training process is set to 3000 episodes. When the target score of `+0.5` is first achieved the agent networks are saved, Then in order to achieve optimal results, each time the score is achieving +0.01 better than the maximum result, the agent networks are overwritten to the new value. The ouput of the program is:\n",
    "\n",
    "\n",
    "``` \n",
    "Episode 100     Average Score: 0.00     Score: -0.00\n",
    "Episode 200     Average Score: 0.00     Score: -0.00\n",
    "Episode 300     Average Score: 0.00     Score: -0.00\n",
    "Episode 400     Average Score: 0.00     Score: -0.00\n",
    "Episode 500     Average Score: 0.01     Score: 0.050\n",
    "Episode 600     Average Score: 0.00     Score: -0.00\n",
    "Episode 700     Average Score: 0.00     Score: -0.00\n",
    "Episode 800     Average Score: 0.01     Score: -0.00\n",
    "Episode 900     Average Score: 0.00     Score: -0.00\n",
    "Episode 1000    Average Score: 0.02     Score: -0.00\n",
    "Episode 1100    Average Score: 0.03     Score: -0.00\n",
    "Episode 1200    Average Score: 0.02     Score: 0.050\n",
    "Episode 1300    Average Score: 0.02     Score: -0.00\n",
    "Episode 1400    Average Score: 0.01     Score: -0.00\n",
    "Episode 1500    Average Score: 0.04     Score: -0.00\n",
    "Episode 1600    Average Score: 0.06     Score: -0.00\n",
    "Episode 1700    Average Score: 0.11     Score: 0.050\n",
    "Episode 1800    Average Score: 0.42     Score: 0.300\n",
    "Episode 1819    Average Score: 0.50     Score: 0.800\n",
    " Agents saved for score 0.50\n",
    "Episode 1821    Average Score: 0.52     Score: 0.75\n",
    " Agents saved for score 0.51\n",
    "Episode 1822    Average Score: 0.52     Score: 0.80\n",
    " Agents saved for score 0.52\n",
    "Episode 1823    Average Score: 0.53     Score: 0.80\n",
    " Agents saved for score 0.53\n",
    "Episode 1827    Average Score: 0.54     Score: 0.75\n",
    " Agents saved for score 0.54\n",
    "Episode 1829    Average Score: 0.55     Score: 0.70\n",
    " Agents saved for score 0.55\n",
    "Episode 1845    Average Score: 0.56     Score: 0.800\n",
    " Agents saved for score 0.56\n",
    "Episode 1900    Average Score: 0.54     Score: 0.150\n",
    "Episode 2000    Average Score: 0.33     Score: 0.250\n",
    "Episode 2100    Average Score: 0.53     Score: 0.750\n",
    "Episode 2111    Average Score: 0.57     Score: 0.75\n",
    " Agents saved for score 0.57\n",
    "Episode 2114    Average Score: 0.58     Score: 0.80\n",
    " Agents saved for score 0.58\n",
    "Episode 2119    Average Score: 0.59     Score: 0.60\n",
    " Agents saved for score 0.59\n",
    "Episode 2164    Average Score: 0.60     Score: 0.800\n",
    " Agents saved for score 0.60\n",
    "Episode 2178    Average Score: 0.61     Score: 0.800\n",
    " Agents saved for score 0.61\n",
    "Episode 2200    Average Score: 0.56     Score: 0.800\n",
    "Episode 2300    Average Score: 0.52     Score: 0.700\n",
    "Episode 2400    Average Score: 0.57     Score: 0.100\n",
    "Episode 2500    Average Score: 0.38     Score: 0.500\n",
    "Episode 2600    Average Score: 0.55     Score: 0.800\n",
    "Episode 2700    Average Score: 0.52     Score: 0.500\n",
    "Episode 2800    Average Score: 0.60     Score: 0.600\n",
    "Episode 2900    Average Score: 0.56     Score: 0.750\n",
    "Episode 3000    Average Score: 0.56     Score: 0.800\n",
    "```\n",
    "\n",
    "The first time the target of `+0.5` is achieved is at the episode `1819`. Then the agents are further improving achieving a score of `+0.61` for which the agents are saved. \n",
    "Graphically, the score evolution is depicted in the following plot, where the blue line shows the result for every epeisode and the red line is the average of the last `100` episodes. \n",
    "After initially achieving the target, there are some bumps where the performance falls below `+0.5` but the agents are recovering staying above the target for most of the time. \n",
    "\n",
    "\n",
    "\n",
    "[<img src=\"images/scores.png\">]()\n",
    "\n",
    "\n",
    "\n",
    "## Example of trained agents\n",
    "After the training the agents cooperate quite well.\n",
    "\n",
    "\n",
    "[<img src=\"images/tennis_trained.gif\">]()\n",
    "\n",
    "\n",
    "## Ideas for Future Work\n",
    "\n",
    "The performance of the agents in this multiagent environment looks satisfactory. Some further research could be done in selecring the hyperparameters and achieve the required performance faster. That would also mean choosing a different network architecture. \n",
    "\n",
    "Moreover, this environment is purely cooperative. As a next step, the algorithm should be tried in a competitive or a hubrid enviroment such as the soccer one, consisting of two teams of two players each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
